{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import jieba\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "UNK_token = 3\n",
    "MAX_LENGTH = 200\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self):\n",
    "        self.word2index = {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, '<UNK>': 3}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>'}\n",
    "        self.n_words = 4 # Count default tokens\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_langs(path, mode):\n",
    "        lang = Lang()\n",
    "        rm = re.compile(r\"\\s+\", re.MULTILINE)\n",
    "\n",
    "        # Read text\n",
    "        raw_text = \"\"\n",
    "        if os.path.isdir(path):\n",
    "            print(\"loading from path...\")\n",
    "            for filename in os.listdir(path):\n",
    "                print(path+filename)\n",
    "                if os.path.isdir(os.path.join(path, filename)): continue\n",
    "                with open(os.path.join(path, filename), encoding='UTF-8', mode='r') as f:\n",
    "                    temp = rm.sub(\"\", f.read())\n",
    "                    temp = re.sub(r\"[『“]\", r\"「\", temp)\n",
    "                    temp = re.sub(r\"[』”]\", r\"」\", temp)\n",
    "                    raw_text += temp\n",
    "        elif os.path.isfile(path):\n",
    "            print(\"loading from file...\")\n",
    "            with open(path, encoding='UTF-8', mode='r') as f:\n",
    "                temp = rm.sub(\"\", f.read())\n",
    "                temp = re.sub(r\"[『“]\", r\"「\", temp)\n",
    "                temp = re.sub(r\"[』”]\", r\"」\", temp)\n",
    "                raw_text += temp\n",
    "        else:\n",
    "            print(\"Invalid file path. Exiting...\" )\n",
    "            os._exit(1)\n",
    "\n",
    "        # for i in cut_sentence_new(raw_text):    print(i)\n",
    "        if mode == 'char':\n",
    "            word_list = list(raw_text)\n",
    "        elif mode == 'word':\n",
    "            word_list = [w for w in jieba.cut(raw_text, cut_all=False)]\n",
    "        else:\n",
    "            print('Non-supported mode for training. Exiting...')\n",
    "            os._exit(1)\n",
    "\n",
    "\n",
    "        # Map char to int / int to char\n",
    "        for word in word_list:\n",
    "            if word not in lang.word2index:\n",
    "                lang.word2index[word] = lang.n_words\n",
    "                lang.word2count[word] = 1\n",
    "                lang.index2word[lang.n_words] = word\n",
    "                lang.n_words += 1\n",
    "            else:\n",
    "                lang.word2count[word] += 1\n",
    "        # print(self.word2count)\n",
    "        # Prepare training data, every <seq_length> sequence, predict 1 char after it\n",
    "        pairs = []\n",
    "        sentences = cut_sentence_new(raw_text)\n",
    "        for ind in range(len(sentences)-1):\n",
    "            pairs.append([sentences[ind], sentences[ind+1]])\n",
    "        return lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_sentence_new(words):\n",
    "    # words = (words).decode('utf8')\n",
    "    start = 0\n",
    "    i = 0\n",
    "    sents = []\n",
    "    closure_flag = False\n",
    "    punt_list = '.!?:;~。！？：；～』”」'\n",
    "    closure_list = \"「“『』”」\"\n",
    "    for word in words:\n",
    "        if word in closure_list:    closure_flag = not (closure_flag)\n",
    "        if word in punt_list and token not in punt_list and not (closure_flag):\n",
    "            # check if next word is punctuation or not\n",
    "            sents.append(words[start:i + 1])\n",
    "            start = i + 1\n",
    "            i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "            token = list(words[start:i + 2]).pop()\n",
    "            # get next word\n",
    "    if start < len(words):\n",
    "        sents.append(words[start:])\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\HAN_SH~1.ASU\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.770 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 75 sentence pairs, total 609 words.\n",
      "['「啊！原來是你呀，游泳健將，」', '小公主對青蛙說道，「我在這兒哭，是因為我的金球掉進水潭裡去了。」']\n"
     ]
    }
   ],
   "source": [
    " def prepare_data(path, mode='word'):\n",
    "        output_lang, pairs = read_langs(path, mode)\n",
    "        print(\"Read {} sentence pairs, total {} words.\".format(len(pairs), output_lang.n_words))\n",
    "\n",
    "        # pairs = self.filter_pairs(pairs)\n",
    "        # print(\"Filtered to %d pairs\" % len(pairs))\n",
    "        # for p in pairs:\n",
    "        #     print(p)\n",
    "        return output_lang, pairs\n",
    "\n",
    "output_lang, pairs = prepare_data('C:/Users/han_shih.ASUS/Documents/story/testing/001.txt', 'word')\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.01, batch_size=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout, bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "#         print(input)\n",
    "#         embedded = self.embedding(input).view(1, 1, -1)\n",
    "#         print(embedded.shape)\n",
    "#         output, hidden = self.gru(embedded, hidden)\n",
    "#         print(output.shape)\n",
    "\n",
    "        embedded = self.embedding(input_seqs)\n",
    "#         print(embedded.size(), input_lengths)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        output, hidden = self.gru(packed, hidden)\n",
    "        output, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(output)\n",
    "        output = output[:, :, :self.hidden_size] + output[:, :, self.hidden_size:]  # Sum bidirectional outputs\n",
    "#         print(output.shape, hidden.shape)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(2, self.batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(this_batch_size, max_len))  # B x S\n",
    "\n",
    "        if USE_CUDA:  attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # For each batch of encoder outputs\n",
    "        for b in range(this_batch_size):\n",
    "            # Calculate energy for each encoder output\n",
    "            for i in range(max_len):\n",
    "                attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
    "        # softmax on timestamp dim\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "\n",
    "    def score(self, hidden, encoder_output):\n",
    "        if self.method == 'dot':\n",
    "            energy = hidden.mm(encoder_output.t())\n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            # print(hidden.size(), energy.size())\n",
    "            energy = hidden.mm(energy.t())\n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = self.v.mm(energy.t())\n",
    "        else:\n",
    "            print(\"undefined scoring strategy!!!\")\n",
    "            return\n",
    "        return energy\n",
    "\n",
    "class BahdanauAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(BahdanauAttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        # self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = Attn('concat', hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "        # TODO: FIX BATCHING\n",
    "\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1)  # S=1 x B x N\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "\n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # B x 1 x N (batch matmul)\n",
    "        context = context.transpose(0, 1)  # 1 x B x N\n",
    "\n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, context), 2)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "\n",
    "        # Final output layer\n",
    "        output = output.squeeze(0)  # B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "\n",
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        if attn_model != 'none':  # Choose attention model\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "#         print(\"decoder forwarding~~~\")\n",
    "#         print(input_seq.shape, last_hidden.shape, encoder_outputs.shape)\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        batch_size = input_seq.size(0)\n",
    "        embedded = self.embedding(input_seq)\n",
    "#         print(batch_size, embedded.shape)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "#         print(batch_size, embedded.shape)\n",
    "        embedded = embedded.view(1, batch_size, self.hidden_size)  # S=1 x B x N\n",
    "#         print(batch_size, embedded.shape)\n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs;\n",
    "        # apply to encoder outputs to get weighted average\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # B x S=1 x N\n",
    "\n",
    "        # Attentional vector using the RNN hidden state and context vector\n",
    "        # concatenated together (Luong eq. 5)\n",
    "        rnn_output = rnn_output.squeeze(0)  # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)  # B x S=1 x N -> B x N\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = F.tanh(self.concat(concat_input))\n",
    "\n",
    "        # Finally predict next token (Luong eq. 6, without softmax)\n",
    "        output = self.out(concat_output)\n",
    "\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list of indexes, one for each word in the sentence, plus EOS\n",
    "def indexes_from_sentence(lang, sentence, mode='word'):\n",
    "    if mode == 'char':\n",
    "        return [lang.word2index[word] for word in sentence]\n",
    "    elif mode == 'word':\n",
    "        return [lang.word2index[word] for word in jieba.cut(sentence, cut_all=False)]\n",
    "    else:\n",
    "        print('Non-supported mode for preprocessing! Exiting...')\n",
    "        os._exit(1)\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(output_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad a with the PAD symbol\n",
    "def pad_seq(seq, max_length):\n",
    "    seq += [PAD_token for i in range(max_length - len(seq))]\n",
    "    return seq\n",
    "\n",
    "def random_batch(lang, pairs, batch_size=5, mode='word'):\n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "\n",
    "    # Choose random pairs\n",
    "    for _ in range(batch_size):\n",
    "        pair = random.choice(pairs)\n",
    "        input_seqs.append(indexes_from_sentence(lang, pair[0], mode))\n",
    "        target_seqs.append(indexes_from_sentence(lang, pair[1], mode))\n",
    "\n",
    "    # Zip into pairs, sort by length (descending), unzip\n",
    "    seq_pairs = sorted(zip(input_seqs, target_seqs), key=lambda p: len(p[0]), reverse=True)\n",
    "    input_seqs, target_seqs = zip(*seq_pairs)\n",
    "\n",
    "    # For input and target sequences, get array of lengths and pad with 0s to max length\n",
    "    input_lengths = [len(s) for s in input_seqs]\n",
    "    input_padded = [pad_seq(s, max(input_lengths)) for s in input_seqs]\n",
    "    target_lengths = [len(s) for s in target_seqs]\n",
    "    target_padded = [pad_seq(s, max(target_lengths)) for s in target_seqs]\n",
    "\n",
    "    # Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n",
    "    input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1).to(device)\n",
    "    target_var = Variable(torch.LongTensor(target_padded)).transpose(0, 1).to(device)\n",
    "\n",
    "    # print(input_var, input_lengths, target_var, target_lengths)\n",
    "    return input_var, input_lengths, target_var, target_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.range(0, max_len - 1).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand).to(device)\n",
    "    # if USE_CUDA:\n",
    "    #     seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1).expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def masked_cross_entropy(logits, target, length):\n",
    "    length = Variable(torch.LongTensor(length)).to(device)\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes), softmax on timestamp dim\n",
    "    log_probs_flat = functional.log_softmax(logits_flat, dim=1)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_batches, input_lengths, target_batches, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "#     input_length = input_tensor.size(0)\n",
    "#     target_length = target_tensor.size(0)\n",
    "\n",
    "#     encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "#     for ei in range(input_length):\n",
    "#         encoder_output, encoder_hidden = encoder(\n",
    "#             input_tensor[ei], encoder_hidden)\n",
    "#         encoder_outputs[ei] = encoder_output[0, 0]        \n",
    "#     decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "#     decoder_hidden = encoder_hidden[:decoder.n_layers] #use last hidden state from encoder\n",
    "\n",
    "    batch_size = len(input_lengths)\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, encoder_hidden)\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token] * batch_size)).to(device)\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "#     print(encoder_hidden.shape, decoder_hidden.shape)\n",
    "    max_target_length = max(target_lengths)\n",
    "    all_decoder_outputs = Variable(torch.zeros(max_target_length, batch_size, decoder.output_size)).to(device)\n",
    "\n",
    "\n",
    "    use_teacher_forcing = True #if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "#         for di in range(target_length):\n",
    "#             decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "#                 decoder_input, decoder_hidden, encoder_outputs)\n",
    "#             loss += criterion(decoder_output, target_tensor[di])\n",
    "#             decoder_input = target_tensor[di]  # Teacher forcing\n",
    "            \n",
    "        for t in range(max(target_lengths)):            \n",
    "            decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "\n",
    "            all_decoder_outputs[t] = decoder_output\n",
    "            decoder_input = target_batches[t] # Next input is current target\n",
    "\n",
    "        # Loss calculation and backpropagation\n",
    "        loss = masked_cross_entropy(\n",
    "            all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "            target_batches.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "            target_lengths\n",
    "        )        \n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "#         for di in range(target_length):\n",
    "#             decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "#                 decoder_input, decoder_hidden, encoder_outputs)\n",
    "#             topv, topi = decoder_output.topk(1)\n",
    "#             decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "#             loss += criterion(decoder_output, target_tensor[di])\n",
    "#             if decoder_input.item() == EOS_token:\n",
    "#                 break\n",
    "\n",
    "    \n",
    "        for t in range(max(target_lengths)):\n",
    "            print(decoder_hidden.shape)\n",
    "            decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            \n",
    "            all_decoder_outputs[t] = decoder_output\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input                        \n",
    "\n",
    "        # Loss calculation and backpropagation\n",
    "        loss = masked_cross_entropy(\n",
    "            all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "            target_batches.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "            target_lengths\n",
    "        )\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.005, batch_size=10):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "#     training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "#                       for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        input_batches, input_lengths, target_batches, target_lengths \\\n",
    "            = random_batch(output_lang, pairs, batch_size=batch_size, mode='word')\n",
    "#         training_pair = training_pairs[iter - 1]\n",
    "#         input_tensor = training_pair[0]\n",
    "#         target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_batches, input_lengths, target_batches, target_lengths, \n",
    "                     encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(output_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = torch.zeros(2, 1, 32, device=device)#encoder.initHidden()\n",
    "        '''\n",
    "        \n",
    "        decoder_input = Variable(torch.LongTensor([SOS_token] * batch_size)).to(device)\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "\n",
    "        max_target_length = max(target_lengths)\n",
    "        all_decoder_outputs = Variable(torch.zeros(max_target_length, batch_size, decoder.output_size)).to(device)\n",
    "\n",
    "        use_teacher_forcing = True #if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "        for t in range(max(target_lengths)):            \n",
    "            decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "\n",
    "            all_decoder_outputs[t] = decoder_output\n",
    "            decoder_input = target_batches[t] # Next input is current target\n",
    "        '''\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor, [input_length], encoder_hidden)\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):            \n",
    "#             decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "#                 decoder_input, decoder_hidden, encoder_outputs)\n",
    "#             decoder_attentions[di] = decoder_attention.data\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = Variable(torch.LongTensor([topi])).to(device)#topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.01 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3m 54s (- 35m 10s) (50 10%) 5.5550\n",
      "7m 46s (- 31m 4s) (100 20%) 4.9777\n",
      "12m 2s (- 28m 6s) (150 30%) 4.4073\n",
      "15m 11s (- 22m 47s) (200 40%) 3.7293\n",
      "18m 38s (- 18m 38s) (250 50%) 3.0362\n",
      "22m 49s (- 15m 12s) (300 60%) 2.3844\n",
      "27m 16s (- 11m 41s) (350 70%) 1.8692\n",
      "32m 11s (- 8m 2s) (400 80%) 1.4220\n",
      "35m 36s (- 3m 57s) (450 90%) 1.0563\n",
      "39m 10s (- 0m 0s) (500 100%) 0.8608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_size = 32\n",
    "batch_size = 10\n",
    "encoder1 = EncoderRNN(output_lang.n_words, hidden_size, 1, batch_size=batch_size).to(device)\n",
    "attn_decoder1 = LuongAttnDecoderRNN('general', hidden_size, output_lang.n_words, 1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 500, print_every=50, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 第二天早上，太陽爬上山的時候，一輛八匹馬拉的大馬車已停在了門前，馬頭上都插著潔白的羽毛，一晃一晃的，馬身上套著金光閃閃的馬具。\n",
      "= 車後邊站著王子的僕人──忠心耿耿的亨利。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "< 車後邊 站 著 王子 的 僕 人 ─ ─ 忠心耿耿 的 亨利 。 」 的 亨利 。 」 的 亨利 金球 ， 你 從 是 樹下 ， 你 從 是 到 了 。 」 的 亨利 水潭 裡 。 」 了 美麗 的 時候 ， 不是 是 那 。 」 的 亨利 的 亨利 。 」 的 太陽 ， 於 是 他 的 太陽 我 ， 明天 的 青蛙 的 僕 感到 感到 感到 回 ， 明天 的 青蛙 的 美麗 的 美麗 的 美麗 的 美麗 的 時候 ， 馬 頭上 他 是 他 是 他 是 你 從 門外 到 了 。 」 ， 不是 的 亨利 青蛙 的 青蛙 ， 快點兒 把門 打開 的 亨利 。 」 ， 別哭 了 。 」 ， 不是 是 那 。 」 ， 別哭 了 。 」 的 時候 ， 馬 車來 我 的 太陽 把 就 得 一下 。 」 ， 馬 爬 到 了 敲門聲 ， 別哭 了 。 」 的 亨利 的 時候 ， 馬 頭上 他 是 到 了 。 」 的 亨利 。 」 的 亨利 ， 於 是 是 是 到 了 這話 ， 不是 是 那 來 ， 接著 他 是 樹下 。 」\n",
      "\n",
      "> 誰知他一落地，已不再是什麼青蛙，卻一下子變成了一位王子，一位兩眼炯炯有神、滿面笑容的王子。\n",
      "= 直到這時候，王子才告訴小公主，原來他被一個狠毒的巫婆施了魔法，除了小公主以外，誰也不能把他從水潭裡解救出來。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "< 車後邊 後 ， 原來 他 主人 又 青蛙 ， 於 是 他 他 從 水潭 裡 了 隻 青蛙 ， 於 是 他 他 從 水潭 裡 。 坐在 喊叫 隻 之 後 ， 原來 他 被 ， 除了 小 公主 以外 ， 誰 也 不能 把 他 從 水潭 裡 。 」 了 魔法 ， 除了 小 公主 以外 ， 誰 也 不能 把 他 從 水潭 裡 。 」 ， 除了 小 公主 以外 ， 誰 也 不能 把 他 從 水潭 裡 。 」 ， 除了 小 公主 以外 ， 除了 小 公主 水潭 裡 。 坐在 水潭 裡 了 隻 魔法 ， 除了 小 公主 水潭 裡 。 」 ， 除了 小 公主 以外 ， 誰 也 不能 把 他 從 水潭 裡 。 」 ， 除了 小 公主 以外 ， 誰 也 不能 把 他 從 水潭 裡 。 」 ， 除了 小 公主 以外 ， 誰 也 不能 把 他 從 水潭 裡 。 」 ， 除了 小 公主 以外 ， 誰 也 不能 把 他 從 水潭 裡 了 三個 好 施 了 魔法 ， 除了 小 公主 以外 ， 他 他 從 水潭 裡 。 現在 把 他 他 ， 除了 小\n",
      "\n",
      "> 「親愛的青蛙，你要什麼東西都成呵，」\n",
      "= 小公主回答說，「我的衣服、我的珍珠和寶石，甚至我頭上戴著的這頂金冠，都可以給你。」\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "< 小 公主 回答 說 ， 「 我 的 衣服 、 我 的 珍珠 和 寶石 ， 甚至 我頭 上 戴 著 的 這頂 金冠 ， 都 可以 給你 。 」 的 這頂 金冠 ， 我頭 上 戴 著 的 這頂 金冠 ， 都 可以 給你 。 」 ， 我頭 上 戴 著 的 這頂 金冠 ， 都 可以 給你 。 」 ， 我頭 上 戴 著 的 這頂 金冠 ， 都 可以 給你 。 」 ， 」 把 我 的 珍珠 和 寶石 ， 甚至 我頭 上 戴 著 的 這頂 金冠 ， 都 可以 給你 。 」 ， 甚至 我頭 上 戴 著 的 這頂 金冠 ， 都 可以 給你 。 」 ， 我頭 上 戴 著 的 這頂 金冠 ， 都 可以 給你 。 」 ， 我頭 上 戴 著 的 這頂 金冠 ， 都 可以 給你 。 」 ， 甚至 我頭 上 戴 著 的 這頂 金冠 ， 都 可以 給你 。 」 ， 我頭 上 戴 著 的 這頂 金冠 ， 都 可以 給你 。 」 ， 我頭 上 戴 著 的 這頂 金冠 ， 都 可以 給你 。 」 ， 我頭 的 和 寶石 ， 甚至 我頭 上 戴 著 的 這頂 金冠 ， 都\n",
      "\n",
      "> 小公主走過去把門打開，青蛙蹦蹦跳跳地進了門，然後跟著小公主來到座位前，接著大聲叫道，「把我抱到你身旁呀！」\n",
      "= 小公主聽了嚇得發抖，國王卻吩咐她照青蛙說的去做。\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "< 她 說 ， 「 他 他 卻 ， 想 卻 吩咐 的 時代 的 去 。 」 了 嚇得 發抖 ， 就 「 在 我們 的 旨意 ， 國王 金球 卻 沒 想 ： 了 。 」 了 嚇得 發抖 ， 就 哭 了 隻 去 做 。 」 了 。 」 金球 掉 到 了 。 」 金球 撈 水潭 裡 。 」 了 。 」 了 嚇得 伸出 去 。 」 了 青蛙 ， 把 卻 卻 做 了 。 」 了 嚇得 發抖 ， 就 「 他 的 青蛙 。 」 」 了 三個 東西壞 了 。 」 金球 撈 了 。 」 ， 把 卻 想 裡的 青蛙 了 。 」 了 嚇得 發抖 ， 就 「 小 公主 。 」 了 嚇得 發抖 ， 就 「 小 公主 得 發抖 ， 想 卻 吩咐 她 照 青蛙 說 ， 「 他 就 了 隻 青蛙 ， 每次 照 聽 了 隻 青蛙 說 ， 「 他 就 了 青蛙 。 」 了 隻 被 。 」 金球 」 ， 想 卻 吩咐 她 照 青蛙 說 了 。 」 了 。 」 」 了 門口 ， 就 哭 了 隻 青蛙 說 ， 「 他\n",
      "\n",
      "> 小公主對青蛙說道，「我在這兒哭，是因為我的金球掉進水潭裡去了。」\n",
      "= 「好啦，不要難過，別哭了，」\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "< 「 好 ， 心裡 曾經 津津有味 ， 心裡 曾經 一點兒 」 了 。 」 ， 心裡 想 青蛙 的 小 公主 來 ， 心裡 的 青蛙 的 王國 。 」 ， 心裡 想 的 時候 ， 你 的 時候 ， 心裡 曾經 轉身 ， 你 的 好 ， 心裡 想 的 時候 ， 你 會 青蛙 的 小 ， 心裡 想 的 時候 ， 心裡 曾經 一點兒 聽見 她 的 時候 ， 」 把 金球 撈 的 王國 。 」 了 。 」 ， 心裡 想 的 ， 」 了 。 」 ， 心裡 害怕 的 ， 」 的 ， 」 一 。 」 ， 心裡 想 的 時候 ， 你 ， 心裡 的 青蛙 的 時候 ， 」 一 。 」 ， 」 一 。 」 ， 」 了 。 」 ， 心裡 怎麼 的 金球 伴侶 ， 心裡 睡 想 的 時候 ， 心裡 對 青蛙 了 ， 」 了 。 」 了 。 」 了 。 」 ， 心裡 曾經 好 的 ， 」 一 。 」 ， 」 一 。 」 ， 」 的 小 ， 心裡 的 青蛙 的 王國 。 」 ， 」 一 。 」 ， 心裡 想 的 青蛙 的 王國 。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"「好的，太好了，」\")\n",
    "plt.matshow(attentions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"je sais\")\n",
    "\n",
    "# evaluateAndShowAttention(\"sans façons !\")\n",
    "\n",
    "# evaluateAndShowAttention(\"Il n'en est pas question !\")\n",
    "\n",
    "# evaluateAndShowAttention(\"En aucune manière !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
