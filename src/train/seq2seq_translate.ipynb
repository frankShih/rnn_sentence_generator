{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import jieba\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "UNK_token = 3\n",
    "MAX_LENGTH = 200\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self):\n",
    "        self.word2index = {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, '<UNK>': 3}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>'}\n",
    "        self.n_words = 4 # Count default tokens\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_langs(path, mode):\n",
    "    lang = Lang()\n",
    "    rm = re.compile(r\"\\s+\", re.MULTILINE)\n",
    "\n",
    "    # Read text\n",
    "    raw_text = \"\"\n",
    "    if os.path.isdir(path):\n",
    "        print(\"loading from path...\")\n",
    "        for filename in os.listdir(path):\n",
    "            print(path+filename)\n",
    "            if os.path.isdir(os.path.join(path, filename)): continue\n",
    "            with open(os.path.join(path, filename), encoding='UTF-8', mode='r') as f:\n",
    "                temp = rm.sub(\"\", f.read())\n",
    "                temp = re.sub(r\"[『“]\", r\"「\", temp)\n",
    "                temp = re.sub(r\"[』”]\", r\"」\", temp)\n",
    "                raw_text += temp\n",
    "    elif os.path.isfile(path):\n",
    "        print(\"loading from file...\")\n",
    "        with open(path, encoding='UTF-8', mode='r') as f:\n",
    "            temp = rm.sub(\"\", f.read())\n",
    "            temp = re.sub(r\"[『“]\", r\"「\", temp)\n",
    "            temp = re.sub(r\"[』”]\", r\"」\", temp)\n",
    "            raw_text += temp\n",
    "    else:\n",
    "        print(\"Invalid file path. Exiting...\" )\n",
    "        os._exit(1)\n",
    "\n",
    "    # for i in cut_sentence_new(raw_text):    print(i)\n",
    "    if mode == 'char':\n",
    "        word_list = list(raw_text)\n",
    "    elif mode == 'word':\n",
    "        word_list = [w for w in jieba.cut(raw_text, cut_all=False)]\n",
    "    else:\n",
    "        print('Non-supported mode for training. Exiting...')\n",
    "        os._exit(1)\n",
    "\n",
    "\n",
    "    # Map char to int / int to char\n",
    "    for word in word_list:\n",
    "        if word not in lang.word2index:\n",
    "            lang.word2index[word] = lang.n_words\n",
    "            lang.word2count[word] = 1\n",
    "            lang.index2word[lang.n_words] = word\n",
    "            lang.n_words += 1\n",
    "        else:\n",
    "            lang.word2count[word] += 1\n",
    "    # print(self.word2count)\n",
    "    # Prepare training data, every <seq_length> sequence, predict 1 char after it\n",
    "    pairs = []\n",
    "    sentences = cut_sentence_new(raw_text)\n",
    "    for ind in range(len(sentences)-1):\n",
    "        pairs.append([sentences[ind], sentences[ind+1]])\n",
    "    return lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_sentence_new(words):\n",
    "    # words = (words).decode('utf8')\n",
    "    start = 0\n",
    "    i = 0\n",
    "    sents = []\n",
    "    closure_flag = False\n",
    "    punt_list = '.!?:;~。！？：；～』”」'\n",
    "    closure_list = \"「“『』”」\"\n",
    "    for word in words:\n",
    "        if word in closure_list:    closure_flag = not (closure_flag)\n",
    "        if word in punt_list and token not in punt_list and not (closure_flag):\n",
    "            # check if next word is punctuation or not\n",
    "            sents.append(words[start:i + 1])\n",
    "            start = i + 1\n",
    "            i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "            token = list(words[start:i + 2]).pop()\n",
    "            # get next word\n",
    "    if start < len(words):\n",
    "        sents.append(words[start:])\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\HAN_SH~1.ASU\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.713 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 75 sentence pairs, total 609 words.\n",
      "['正說著話的當兒，又聽見了敲門聲，接著是大聲的喊叫：', '「小公主啊我的愛，快點兒把門打開！愛你的人已到來，快點兒把門打開！你不會忘記昨天，老椴樹下水潭邊，潭水深深球不見，是你親口許諾言。」']\n"
     ]
    }
   ],
   "source": [
    " def prepare_data(path, mode='word'):\n",
    "        output_lang, pairs = read_langs(path, mode)\n",
    "        print(\"Read {} sentence pairs, total {} words.\".format(len(pairs), output_lang.n_words))\n",
    "\n",
    "        # pairs = self.filter_pairs(pairs)\n",
    "        # print(\"Filtered to %d pairs\" % len(pairs))\n",
    "        # for p in pairs:\n",
    "        #     print(p)\n",
    "        return output_lang, pairs\n",
    "\n",
    "output_lang, pairs = prepare_data('C:/Users/han_shih.ASUS/Documents/story/testing/001.txt', 'word')\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.01, batch_size=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout, bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "#         print(input)\n",
    "#         embedded = self.embedding(input).view(1, 1, -1)\n",
    "#         print(embedded.shape)\n",
    "#         output, hidden = self.gru(embedded, hidden)\n",
    "#         print(output.shape)\n",
    "\n",
    "        embedded = self.embedding(input_seqs)\n",
    "#         print(embedded.size(), input_lengths)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        output, hidden = self.gru(packed, hidden)\n",
    "        output, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(output)\n",
    "        output = output[:, :, :self.hidden_size] + output[:, :, self.hidden_size:]  # Sum bidirectional outputs\n",
    "#         print(output.shape, hidden.shape)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(2, self.batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(this_batch_size, max_len))  # B x S\n",
    "\n",
    "        if USE_CUDA:  attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # For each batch of encoder outputs\n",
    "        for b in range(this_batch_size):\n",
    "            # Calculate energy for each encoder output\n",
    "            for i in range(max_len):\n",
    "                attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
    "        # softmax on timestamp dim\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "\n",
    "    def score(self, hidden, encoder_output):\n",
    "        if self.method == 'dot':\n",
    "            energy = hidden.mm(encoder_output.t())\n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            # print(hidden.size(), energy.size())\n",
    "            energy = hidden.mm(energy.t())\n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = self.v.mm(energy.t())\n",
    "        else:\n",
    "            print(\"undefined scoring strategy!!!\")\n",
    "            return\n",
    "        return energy\n",
    "\n",
    "class BahdanauAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(BahdanauAttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        # self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = Attn('concat', hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "        # TODO: FIX BATCHING\n",
    "\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1)  # S=1 x B x N\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "\n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # B x 1 x N (batch matmul)\n",
    "        context = context.transpose(0, 1)  # 1 x B x N\n",
    "\n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, context), 2)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "\n",
    "        # Final output layer\n",
    "        output = output.squeeze(0)  # B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "\n",
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        if attn_model != 'none':  # Choose attention model\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "#         print(\"decoder forwarding~~~\")\n",
    "#         print(input_seq.shape, last_hidden.shape, encoder_outputs.shape)\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        batch_size = input_seq.size(0)\n",
    "        embedded = self.embedding(input_seq)\n",
    "#         print(batch_size, embedded.shape)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "#         print(batch_size, embedded.shape)\n",
    "        embedded = embedded.view(1, batch_size, self.hidden_size)  # S=1 x B x N\n",
    "#         print(batch_size, embedded.shape)\n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs;\n",
    "        # apply to encoder outputs to get weighted average\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # B x S=1 x N\n",
    "\n",
    "        # Attentional vector using the RNN hidden state and context vector\n",
    "        # concatenated together (Luong eq. 5)\n",
    "        rnn_output = rnn_output.squeeze(0)  # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)  # B x S=1 x N -> B x N\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = F.tanh(self.concat(concat_input))\n",
    "\n",
    "        # Finally predict next token (Luong eq. 6, without softmax)\n",
    "        output = self.out(concat_output)\n",
    "\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list of indexes, one for each word in the sentence, plus EOS\n",
    "def indexes_from_sentence(lang, sentence, mode='word'):\n",
    "    if mode == 'char':\n",
    "        return [lang.word2index[word] for word in sentence]\n",
    "    elif mode == 'word':\n",
    "        return [lang.word2index[word] for word in jieba.cut(sentence, cut_all=False)]\n",
    "    else:\n",
    "        print('Non-supported mode for preprocessing! Exiting...')\n",
    "        os._exit(1)\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(output_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad a with the PAD symbol\n",
    "def pad_seq(seq, max_length):\n",
    "    seq += [PAD_token for i in range(max_length - len(seq))]\n",
    "    return seq\n",
    "\n",
    "def random_batch(lang, pairs, batch_size=5, mode='word'):\n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "\n",
    "    # Choose random pairs\n",
    "    for _ in range(batch_size):\n",
    "        pair = random.choice(pairs)\n",
    "        input_seqs.append(indexes_from_sentence(lang, pair[0], mode))\n",
    "        target_seqs.append(indexes_from_sentence(lang, pair[1], mode))\n",
    "\n",
    "    # Zip into pairs, sort by length (descending), unzip\n",
    "    seq_pairs = sorted(zip(input_seqs, target_seqs), key=lambda p: len(p[0]), reverse=True)\n",
    "    input_seqs, target_seqs = zip(*seq_pairs)\n",
    "\n",
    "    # For input and target sequences, get array of lengths and pad with 0s to max length\n",
    "    input_lengths = [len(s) for s in input_seqs]\n",
    "    input_padded = [pad_seq(s, max(input_lengths)) for s in input_seqs]\n",
    "    target_lengths = [len(s) for s in target_seqs]\n",
    "    target_padded = [pad_seq(s, max(target_lengths)) for s in target_seqs]\n",
    "\n",
    "    # Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n",
    "    input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1).to(device)\n",
    "    target_var = Variable(torch.LongTensor(target_padded)).transpose(0, 1).to(device)\n",
    "\n",
    "    # print(input_var, input_lengths, target_var, target_lengths)\n",
    "    return input_var, input_lengths, target_var, target_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.range(0, max_len - 1).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand).to(device)\n",
    "    # if USE_CUDA:\n",
    "    #     seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1).expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def masked_cross_entropy(logits, target, length):\n",
    "    length = Variable(torch.LongTensor(length)).to(device)\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes), softmax on timestamp dim\n",
    "    log_probs_flat = functional.log_softmax(logits_flat, dim=1)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_batches, input_lengths, target_batches, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "#     input_length = input_tensor.size(0)\n",
    "#     target_length = target_tensor.size(0)\n",
    "\n",
    "#     encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "#     for ei in range(input_length):\n",
    "#         encoder_output, encoder_hidden = encoder(\n",
    "#             input_tensor[ei], encoder_hidden)\n",
    "#         encoder_outputs[ei] = encoder_output[0, 0]        \n",
    "#     decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "#     decoder_hidden = encoder_hidden[:decoder.n_layers] #use last hidden state from encoder\n",
    "\n",
    "    batch_size = len(input_lengths)\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, encoder_hidden)\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token] * batch_size)).to(device)\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "#     print(encoder_hidden.shape, decoder_hidden.shape)\n",
    "    max_target_length = max(target_lengths)\n",
    "    all_decoder_outputs = Variable(torch.zeros(max_target_length, batch_size, decoder.output_size)).to(device)\n",
    "\n",
    "\n",
    "    use_teacher_forcing = True #if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "#         for di in range(target_length):\n",
    "#             decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "#                 decoder_input, decoder_hidden, encoder_outputs)\n",
    "#             loss += criterion(decoder_output, target_tensor[di])\n",
    "#             decoder_input = target_tensor[di]  # Teacher forcing\n",
    "            \n",
    "        for t in range(max(target_lengths)):            \n",
    "            decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "\n",
    "            all_decoder_outputs[t] = decoder_output\n",
    "            decoder_input = target_batches[t] # Next input is current target\n",
    "\n",
    "        # Loss calculation and backpropagation\n",
    "        loss = masked_cross_entropy(\n",
    "            all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "            target_batches.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "            target_lengths\n",
    "        )        \n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "#         for di in range(target_length):\n",
    "#             decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "#                 decoder_input, decoder_hidden, encoder_outputs)\n",
    "#             topv, topi = decoder_output.topk(1)\n",
    "#             decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "#             loss += criterion(decoder_output, target_tensor[di])\n",
    "#             if decoder_input.item() == EOS_token:\n",
    "#                 break\n",
    "\n",
    "    \n",
    "        for t in range(max(target_lengths)):\n",
    "#             print(decoder_hidden.shape)\n",
    "            decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            \n",
    "            all_decoder_outputs[t] = decoder_output\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input                        \n",
    "\n",
    "        # Loss calculation and backpropagation\n",
    "        loss = masked_cross_entropy(\n",
    "            all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "            target_batches.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "            target_lengths\n",
    "        )\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.005, batch_size=10):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "#     training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "#                       for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        input_batches, input_lengths, target_batches, target_lengths \\\n",
    "            = random_batch(output_lang, pairs, batch_size=batch_size, mode='word')\n",
    "#         training_pair = training_pairs[iter - 1]\n",
    "#         input_tensor = training_pair[0]\n",
    "#         target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_batches, input_lengths, target_batches, target_lengths, \n",
    "                     encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(output_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = torch.zeros(2, 1, encoder.hidden_size, device=device)#encoder.initHidden()\n",
    "        '''\n",
    "        \n",
    "        decoder_input = Variable(torch.LongTensor([SOS_token] * batch_size)).to(device)\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "\n",
    "        max_target_length = max(target_lengths)\n",
    "        all_decoder_outputs = Variable(torch.zeros(max_target_length, batch_size, decoder.output_size)).to(device)\n",
    "\n",
    "        use_teacher_forcing = True #if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "        for t in range(max(target_lengths)):            \n",
    "            decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "\n",
    "            all_decoder_outputs[t] = decoder_output\n",
    "            decoder_input = target_batches[t] # Next input is current target\n",
    "        '''\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor, [input_length], encoder_hidden)\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):            \n",
    "#             decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "#                 decoder_input, decoder_hidden, encoder_outputs)\n",
    "#             decoder_attentions[di] = decoder_attention.data\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = Variable(torch.LongTensor([topi])).to(device)#topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.01 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3m 0s (- 27m 2s) (50 10%) 5.7542\n",
      "7m 2s (- 28m 9s) (100 20%) 5.2006\n",
      "10m 18s (- 24m 3s) (150 30%) 4.9239\n",
      "13m 14s (- 19m 51s) (200 40%) 4.6287\n",
      "17m 21s (- 17m 21s) (250 50%) 4.3127\n",
      "21m 29s (- 14m 19s) (300 60%) 4.0093\n",
      "25m 10s (- 10m 47s) (350 70%) 3.7505\n",
      "28m 34s (- 7m 8s) (400 80%) 3.4861\n",
      "31m 48s (- 3m 32s) (450 90%) 3.2369\n",
      "35m 32s (- 0m 0s) (500 100%) 3.0382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD9CAYAAABHnDf0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvFvnyVgAAIABJREFUeJzt3Xd8lFXa//HPlUYglAAJUkIRaUoJvXdEURFEEctiB2yAApZ112fd1d31EUEQFJBVaS4qgiCyAqL0KkGkd+mCVEE6Cef3R8Z9+MUAA0xyz0y+79crLye5T+75eutcMzlz5lzmnENERMJLhNcBREQk8FTcRUTCkIq7iEgYUnEXEQlDKu4iImFIxV1EJAxF+TPIzLYBvwJpQKpzrnYmY5oDA4Fo4IBzrlngYoqIyOXwq7j7tHDOHcjsgJnFA0OANs65HWZWJCDpRETkigRqWuZ+4HPn3A4A59y+AJ1XRESugL/F3QFfm9kyM+uWyfEKQEEzm+0b82DgIoqIyOXyd1qmkXPuJ990ywwzW++cm5vhPLWAVkBuYJGZLXbObTz/JL4nhm4AcXFxtSpVqnT1/wYiIjnIsmXLDjjnEi81zq/i7pz7yffPfWY2EagLnF/cd5H+Jupx4LiZzQWSgY0ZzjMcGA5Qu3Ztl5KS4s/di4iIj5lt92fcJadlzCzOzPL9dhu4CVidYdgXQBMzizKzPEA9YN3lRRYRkUDx55X7NcBEM/tt/Fjn3DQzewLAOTfMObfOzKYBK4FzwPvOuYxPACIikk3Mqy1/NS0jInL5zGxZZp81ykifUBURCUMq7iIiYUjFXUQkDKm4i4iEoZAr7oeOn+HVL9dy4kyq11FERIJWyBX3BZsPMGLhVjq8u5CtB457HUdEJCiFXHG/Pbk4ox+ty75fT9Fu8Hymr9nrdSQRkaATcsUdoEn5RKb0bELZxDgeH7OM/526ntS0c17HEhEJGn4VdzPbZmarzOwHM7vgJ4/MrI6ZpZlZx8BFzFyJ+NyMe6IB99crxbA5W3jww+84cOx0Vt+tiEhIuJxX7i2cc9Uv9MkoM4sE3gCmBySZH3JFRfLPDlXpd3cyy7Yfpu2g+Xy/43B23b2ISNAK5LRMD2ACkO2NOjrWSuLzpxoSExXBPe8tYvSibXi1rYKISDAISLMOMysBdACGBTLc5ahcvABfdm9Mk/KJ/OWLNfQet0LLJUUkx/K3uDdyztUEbgGeNrOmGY4PBF50zqVd7CRm1s3MUswsZf/+/VcQ9+IK5Inm/Qdr06d1BSb9sJs7h2i5pIjkTJe9K6SZ/RU45pzrd97PtgLm+zYBOAF0c85NutB5snpXyDkb9/PMJ8tJS3P075TMTZWLZtl9iYhkl4DtCulPsw7n3LXOuTLOuTLAeOCpixX27NCsQiJTejTm2sQ4uo1ZxhvTtFxSRHIOf6ZlrgHmm9kK4DvgP7816/itYUewSiqYh3GPN+C+uqUYOlvLJUUk58gxzTrGpezkfyatplBcDEP+UJMapQpm232LiASKmnVk0Kl2SSY82ZCoSKPTe4sYs3i7lkuKSNjKMcUdoEqJAkzp3oTG5RL4n0mr6TNuBSfPXHSBj4hISMpRxR3Sl0t+8FAdereuwMQfdtNhyAK2abmkiISZHFfcASIijJ6tyjPykbrsPXqK29+Zz4y1P3sdS0QkYHJkcf9NswqJfNm9MWUKx9F1dApvTl9P2jnNw4tI6MvRxR2gZKE8fPZEA+6tU5J3Z23hoQ+/46CWS4pIiMvxxR0gNjqS/72rGn3vqsZ32w5x++D5/LDzF69jiYhcMRX383SqU5LPn2xIRIRx97CFfKTlkiISogLSrMPM/mBmK31fC80sOfBRs0eVEgWY0qMxjcol8PKk1fT5TMslRST0BKpZx1agmXOuGvAaMDwg6TwSnyeGDx+qw7M3lmfi8vTlktsParmkiISOgEzLOOcWOud+a4G0GEgKxHm9FBFhPHtjBUY8XIc9R07RdvB8vtFySREJEQFp1pHBY8DUq4sVPJpXLMKUHo0pXTgPXUan0G/6Bi2XFJGgF6hmHQCYWQvSi/uLFziepc06skrJQnkY/0RD7qldkndmbebhEd9x6PgZr2OJiFyQX8XdOfeT75/7gIlA3YxjzKwa8D7Q3jl38ALnGe6cq+2cq52YmHjlqT0QGx3JGx2r8cZdVVmyNX255AotlxSRIBWQZh1mVgr4HHjAObcxK4IGi3vqlGLCEw0BuHvYIv69RMslRST4BKpZx1+AwsCQCy2XDCdVk9KXSza4rjB/nria5z5byamzWi4pIsEjxzTryApp5xyDvt3EoJmbuL5ofoZ1rkWpwnm8jiUiYUzNOrJBZITRq3UFPnyoDrt/OUnbwfP4dp2WS4qI91TcA6BFpfTlkiUL5eGxUSn0/1rLJUXEWyruAVKyUB4mPNmQTrWTGDxTyyVFxFsq7gEUGx1J347J/O+dWi4pIt5Scc8C99YtxfgnGgDpyyXHLtmh5ZIikq1U3LNItaR4pvRoTP3rCvOniat4YbyWS4pI9lFxz0IF42IY8XAderYqz2fLdnHX0IXsOHjC61gikgOouGexyAijd+sKfPhwbXYeOkHbwfOYtX6f17FEJMwFqlmHmdkgM9vsa9hRM/BRQ1vLStcwpUcTkgrm4ZGRS3lrxkYtlxSRLBOoZh23AOV9X92AoYEIF25KFc7D5081pGOtJAZ9u4lHRi7lsJZLikgWCNS0THtgtEu3GIg3s2IBOndYiY2O5M2O1fhnh6os3nKQtoPns3KXlkuKSGAFqllHCWDned/v8v1MMmFm3F+vFJ/5lkt2HLqIT77b4XEqEQkngWrWYZn8zu8mlEO1WUdWSS4Zz5c9GlOvbCH++PkqXhi/QsslRSQgAtWsYxdQ8rzvk4CfMjlPyDbryCqF4mIY+UhderQsx7iU9OWSOw9puaSIXJ2ANOsAJgMP+lbN1AeOOOf2BDxtmIqMMPrcVJEPHvptueR8LZcUkasSqGYdXwE/ApuBfwFPZUnaMNfq+vTlksXjc/PoKC2XFJErp2YdQejU2TT+PHE1E77fRbMKiQy8pzoF42K8jiUiQUDNOkJYbHQk/e6uxj86VGGRb7nkql1HvI4lIiFExT1ImRl/qFeacU80wDnHXcMW8ulSLZcUEf+ouAe56iXjmdKzCXXLFOLFCat4UbtLiogfVNxDQKG4GEY9WpfuLcrxacpOOg7TckkRuTgV9xARGWE8d3NF3n+wNtsPnuD2d+Yze4OWS4pI5lTcQ8yNN1zDl90bUzR/LI+MXMrAbzZyTsslRSQDFfcQVCYhjolPNaJDjRIM/GYTj45ayi8ntLukiPwfFfcQlTsmkv53J/P3O6qwYPMB2g6ez+rdWi4pIun8Lu5mFmlmy81sSibHSpnZLN/xlWZ2a2BjSmbMjM71SzPu8QaknXPcOXQh45buvPQvikjYu5xX7s8A6y5w7GVgnHOuBnAvMORqg4n/apQqyJQejalTpiAvTFjJHydouaRITudvm70k4Dbg/QsMcUB+3+0CZLIjpGStwnlzMfrRejzV/Do+WbqTu4ct0nJJkRzM31fuA4EXgHMXOP5XoLOZ7SJ9E7EemQ3Sfu5ZKzLCeKFNJYY/UIttB45ruaRIDubPlr9tgX3OuWUXGXYfMNI5lwTcCowxs9+dW/u5Z4+bKhflyx7/t1zy7W82aXdJkRzGn1fujYB2ZrYN+ARoaWYfZRjzGDAOwDm3CIgFEgKYUy7Tb8sl76heggHfbOS+4YvZcVDTNCI5xSWLu3PuJedcknOuDOlvls50znXOMGwH0ArAzK4nvbhr3sVjuWMieatTMv3uTmbdnqO0eXsuY5fswKttnkUk+1zxOncze9XM2vm+7QN09TX0+Bh42KmCBAUzo2OtJKb1akqNUvH8aeIqHhm5lJ+PnvI6mohkITXryEHOnXOMWbyd16euI1dUJK+2r0y75OKYZdbfXESCkZp1yO9ERBgPNSzDVz2bUDYxjmc++YHuY5dz6Li2LhAJNyruOVDZxLyMf6IhL7SpyNdr93LTgLl8s/Znr2OJSACpuOdQkRHGU83LMbl7YxLyxtBldArPf7aCX0+d9TqaiASAinsOd32x/Ezu3pinW1zHhO930WbgPBZuOeB1LBG5SiruQkxUBM/fXInxTzYkJiqC+/+1hL9OXsPJM9qfRiRUqbjLf9UsVZCvejbh4YZlGLlwG7cNmsfyHYe9jiUiV0DFXf4/uWMi+Wu7yvy7Sz1OnU3jrqEL6Td9A2dSL7StkIgEIxV3yVSjcglM69WUO2sm8c6szdzx7gLW7z3qdSwR8VNAmnX4jncys7VmtsbMxgYuonglf2w0/e5O5l8P1mbfr6e4ffB8hszerE3IREJAQJp1mFl54CWgkXOuMvBsALJJkGh9wzV83asZrW+4hr7TNnD3sIVsPXDc61gichGBatbRFXjXOXcYwDmnTcTDTKG4GN69vyZv31udzfuOcevb8xi9aBvn9CpeJCgFqllHBaCCmS0ws8Vm1iazQWrWEdrMjPbVS/B1r2bUubYQf/liDQ9++B0//XLS62gikkGgmnVEAeWB5qQ37njfzOIzDlKzjvBQtEAsox6pwz86VOH7HYe5eeBcJizbpa2ERYJIoJp17AK+cM6ddc5tBTaQXuwlTJkZf6hXmqnPNKFS0Xz0+WwFj49ZxoFjp72OJiIErlnHJKAFgJklkD5N82OAs0oQKl04jk+6NeBPt1Zi9ob93DxgLtNW7/U6lkiOF6hmHdOBg2a2FpgFPO+cOxiIgBL8IiOMbk2vY0rPxhSLj+WJj5bR+9MfOHJSm5CJeEXNOiSgzqad452Zm3ln1maK5MtF347VaFJe76+IBIqadYgnoiMj6NW6AhOfakhcrige+OA7Xp60ihNnUr2OJpKjqLhLlqiWFM+UHo3p0vha/r1kB7e8PY+UbYe8jiWSY6i4S5aJjY7k5bY38HHX+qSdc3R6bxGvT13H6VRtJSyS1VTcJcvVL1uYac825Z46pXhvzo+0G7yA1buPeB1LJKypuEu2yJsritfvrMqIh+tw+MQZ7nh3AYO/3URqmrYSFskKKu6SrVpUKsLXvZpya9Vi9J+xkbuGLmTzvmNexxIJOyruku3i88Qw6L4avHt/TXYcOsFtg+bx4fyt2oRMJIBU3MUzt1UrxvReTWlcLoFXp6zl/vcXs/PQCa9jiYSFgDXr8I3paGbOzC65wF4EoEi+WN5/qDZ976rG6t1HueXteXy6dIc2IRO5SgFp1gFgZvmAnsCSqw0lOYuZ0alOSaY+04QqJfLz4oRVdBmVwr6jp7yOJhKyAtWsA+A1oC+gR6RckZKF8jC2S31euf0G5m8+wE0D5zJl5U9exxIJSQFp1mFmNYCSzrkLTtn4xqlZh1xURITxSKNr+U/PJpQuHEf3scvp8fFyfjlxxutoIiHlqpt1mFkEMADoc6lzqVmH+KtckbxMeKIBz91Ugamr9nDTgLnMWq/ujSL+CkSzjnxAFWC2b0x9YLLeVJWrFRUZQfeW5fmieyMK5onhkZFLeenzlRw7rU3IRC7lsrb8NbPmwHPOubYXGTPbN+ai+/lqy1+5HKdT0xgwYxPD526heHxu+t2dTP2yhb2OJZLtsnzL3wzNOkSyVK6oSP54SyU+e6IBkRHGff9azGtT1nLqrDYhE8mMmnVIyDlxJpXXv1rPmMXbKVckL291SqZa0u/6sYuEJTXrkLCVJyaK1+6owpjH6nLsVCodhizkrRkbOatNyET+S8VdQlaT8olM79WU9snFGfTtJjoMWcDGn3/1OpZIUFBxl5BWIHc0b91TnWGda7Hnl1O0HTyf4XO3kKZNyCSHU3GXsNCmSlGm92pKi4qJ/POr9dw7fBHbDx73OpaIZ1TcJWwk5M3FsM61eKtTMuv3/sotb8/jo8XbtQmZ5Egq7hJWzIw7ayYx/dmm1CpdkJcnreahEUvZe0RbHknOouIuYal4fG5GP1qX19pXZunWQ9w0YA6Tlu/Wq3jJMQKyn7uZ9TaztWa20sy+NbPSgY0pcvnMjAcalGHqM00of00+nv30B5769/ccPHba62giWS5Q+7kvB2o756oB40nf+lckKJRJiGPc4w14sU0lvl23j5sHzmXG2p+9jiWSpQKyn7tzbpZz7rf+aIuBpMDEEwmMyAjjyebXMblHI4rki6Xr6BSe+2wFR0+d9TqaSJYIyH7uGTwGTL3iRCJZqFLR/Ex6uhE9Wpbj8+930WbAXBZuPuB1LJGAu+r93DOM7QzUBt68wHE16xDPxURF0Oemikx4siGxMZHc//4S/jp5DSfPaBMyCR+X3DjMzF4HHgBSgVggP/C5c65zhnE3AoOBZs65S3ZV0MZhEgxOnkmj7/T1jFiwjWsT4ujfKZmapQp6HUvkggK2cZhz7iXnXJJzrgxwLzAzk8JeA3gPaOdPYRcJFrljInnl9sqM7VqPM6nn6Dh0IX2nred0ql7FS2gL1H7ubwJ5gc/M7AczmxyQdCLZpOF1CUx7tgkdayUxZPYW2gycx5yNmjqU0KX93EUymLtxP3+dvIYfDxznlipFebntDZSIz+11LBFA+7mLXLGmFRKZ+mwTnr+5IrM27OPG/nMYMnszZ1K1X7yEDhV3kUzkiork6Rbl+KZ3M5pWSKDvtA20eXsu8zdp2aSEBhV3kYtIKpiH9x6ozYhH6pB2ztH5gyU8PfZ79hw56XU0kYtScRfxQ4uKRZj+bFN6t67AN2t/plX/Obw3Z4umaiRoqbiL+Ck2OpKercrzTe9mNLwugdenrufWQfNYuEVTNRJ8VNxFLlPJQnl4/6HafPBQbU6npnH/v5bQ8+Pl/HxUe8ZL8FBxF7lCra6/hhm9mvFMq/JMW7OXVv3n8P68Hzmbpqka8Z6Ku8hViI2OpFfrCszo1ZQ6ZQry9/+so+2g+Sz58aDX0SSHC1Szjlxm9qmZbTazJWZWJpAhRYJd6cJxfPhwHYY/UItjp1O5Z/hien36A/t+1VSNeCNQzToeAw4758oBA4A3rjaYSKgxM26qXJRvejejR8ty/GflHlr1m8OIBVtJ1VSNZLOANOsA2gOjfLfHA63MzK4+nkjoyR0TSZ+bKjK9V1NqlC7I375cS9vB80nZdsjraJKDBKpZRwlgJ4BzLhU4AhS+6nQiIezahDhGPVKHYZ1rcvTkWToOW0SfcSs4oB6ukg0C1awjs1fpv9uRTM06JKcxM9pUKcY3fZqlt/lbsZsW/WYzetE20s55s2mf5Az+vHJvBLQzs23AJ0BLM/sow5hdQEkAM4sCCgC/+xvUOTfcOVfbOVc7MTHxqoKLhJI8MVG82KYSU59pSnJSPH/5Yg3t3pnPsu2HvY4mYSogzTqAycBDvtsdfWP0skQkg3JF8jLmsbq8e39NDh47w11DF/LC+BUc1FSNBFigmnV8ABQ2s81Ab+CPgQgnEo7MjNuqFePbPs14vGlZPv9+Ny37z+Gjxds1VSMBo2YdIh7b9POv/OWLNSz68SDVkgrwWvsqJJeM9zqWBCk16xAJEeWvycfYrvUYdF8N9h45xR1DFvDS56s4fPyM19EkhKm4iwQBM6NdcnG+7dOMxxpdy7iUnbToP5uPv9vBOU3VyBVQcRcJIvlio3m57Q181bMJFa7Jx0ufr6LD0IWs2nXE62gSYlTcRYJQxaL5+LRbfQbck8zuwydp9+58Xp60il9OaKpG/KPiLhKkzIwONZKY+VwzHm5YhrFLdtCy/xzGLd2pqRq5JBV3kSCXPzaaV26vzJQeTSibEMcLE1bScdhCVu/WVI1cmIq7SIi4oXh+xj3egH53J7P94AnavTOfV75YzZGTZ72OJkFIxV0khEREGB1rJTHzueY8UL80YxZvp1X/2Yxftgt9KFzO58/GYbFm9p2ZrTCzNWb2t0zGlDKzWb5mHivN7NasiSsiAAVyR/O39lWY3L0xJQvl4bnPVtDpvUWs23PU62gSJPx55X4aaOmcSwaqA23MrH6GMS8D45xzNUjff2ZIYGOKSGaqlCjAhCca0veuamzZf5y2g+fzty/XcPSUpmpyOn82DnPOuWO+b6N9Xxn//nNAft/tAsBPAUsoIhcVEWF0qlOSmX2acW+dkoxcuI1W/ecwafluTdXkYP52Yoo0sx+AfcAM59ySDEP+CnQ2s13AV0CPgKYUkUuKzxPDPzpU5YunG1G8QCzPfvoD9wxfzIa9v3odTTzgV3F3zqU556oDSUBdM6uSYch9wEjnXBJwKzDGzH53bjXrEMl61ZLimfhUI16/syobf/6VWwfN4x//Wcux06leR5NsdNm7QprZK8Bx51y/8362BmjjnNvp+/5HoL5zbt+FzqNdIUWy3qHjZ3hz+no+WbqTIvly8efbbuD2asVQi+PQFbBdIc0s0czifbdzAzcC6zMM2wG08o25HogF9NJcxGOF4mJ4/c5qfP5kQ4rki6Xnx8u5/19L2PSzpmrCnT/TMsWAWWa2ElhK+pz7lAzNOvoAXc1sBfAx8LA6MYkEjxqlCjLp6Ub8/Y4qrN1zlFvensfrU9dxXFM1YUvNOkRymIPHTvPGtPWMS9lF0fyx/E/bG7i1alFN1YQINesQkUwVzpuLvh2TmfBkQwrFxfD02O954IPv2LL/2KV/WUKGirtIDlWrdEG+7NGYV9tXZsWuX2gzcC59p63nxBlN1YQDFXeRHCwywniwQRlm9mlOu+QSDJm9hRv7z2Ha6j36AFSIU3EXERLz5aJ/p2TGPd6A/LmjeeKj73loxFK2HjjudTS5QiruIvJfda8txJQejflL2xv4fvthbh4wl/5fb+DkmTSvo8llUnEXkf9PVGQEjza+lpl9mnFbtWIMnrmZ1gPmMGPtz5qqCSEq7iKSqSL5YxlwT3U+6VafPDGRdB2dwmOjUth+UFM1oUDFXUQuqn7ZwvynZxNevu16lvx4kNYD5jJgxkZOndVUTTALSLMO37hOZrbWN2Zs4KOKiFeiIyPo0qQsM59rTpvKRXn7203cNGAuM9f/7HU0uYCANOsws/LAS0Aj51xl4NmAJxURz12TP5ZB99VgbJd6xERF8OjIFLqMSmGbVtUEnUA16+gKvOucO+z7nQvuBikioa9huQS+6tmEl26pxMItB7jxrTm8PGkV+46e8jqa+ASqWUcFoIKZLTCzxWbWJtBBRSS4xERF8Hiz65j9XHPuq1uKT77bSbM3Z/Pm9PVq8xcELmvjMN/WvxOBHs651ef9fApwFuhEekOPeUAV59wvGX6/G9ANoFSpUrW2b99+1f8CIhIcth04Tv8ZG/lyxU/E54nm6ebleKBBaWKjI72OFlayZOMwX7GeDWR8Zb4L+MI5d9Y5txXYAJTP5PeHO+dqO+dqJyYmXs5di0iQK5MQx+D7ajClR2OqJcXzj6/W0aLfbMYt3Ulq2jmv4+U4gWrWMQlo4RuTQPo0zY+BjSoioaBKiQKMfrQuY7vWo0j+WF6YsJI2b89j2uq9+hBUNgpUs47pwEEzWwvMAp53zh3MmsgiEgoaXpfApKcaMqxzTc45xxMfLaPDkIUs2qLSkB3UrENEslxq2jkmfL+LATM2sffoKZpVSOSFNhWpXLyA19FCjr9z7iruIpJtTp1NY/Sibbw7awtHTp6lXXJx+txUgdKF47yOFjJU3EUkaB05eZb35mzhwwVbSU1z3F+vFN1blqNIvlivowU9FXcRCXr7jp7i7W838cnSneSKiuCxxtfStWlZ8sdGex0taKm4i0jI2HrgOP2/3sCUlXsomCeap1uUo3N9rZHPjIq7iIScVbuO0Hf6euZtOkCJ+Nw8e2N57qyZRGSEeR0taGTJh5hERLJS1aQCjHmsHmO71CMhbwzPj19Jm4Fz+XqN1shfLhV3EQk6DcslMOnpRgz5Q03Szjm6jVnGXUMXsuRHrZH3l4q7iAQlM+PWqsX4uldTXr+zKrt/Ock9wxfzyIjvWLfnqNfxgl7AmnX4xnY0M2dml5wPEhHxR1RkBPfVLcWc51vwx1sqsWz7YW4dNI9nP1nOzkMnvI4XtC75hqqZGRDnnDtmZtHAfOAZ59ziDOPyAf8BYoDuzrmLvluqN1RF5EocOXGWYXO3MGLBVtLOOf5QrzRPtyhHYr5cXkfLFgF7Q9XPZh0ArwF9Ae3WLyJZpkCeaF5sU4k5z7egY62SjFm8nWZvzuKtGRv5VfvI/1dAmnWYWQ2gpHNuShZkFBH5nWvyx/L6nVWZ0aspLSoWYdC3m2j25mw+mL+V06lq3u1XcXfOpTnnqpPeiKOumVX57ZiZRQADgD6XOo+ZdTOzFDNL2b9//5VmFhH5r7KJeXn3DzWZ3L0RNxTLz2tT1tKy3xzGL9tF2rmcu3zysj/EZGavAMedc/183xcAtgC/Td0UBQ4B7S427645dxHJCvM3HeCNaetZtfsIFa/Jx/M3V6TV9UVIf/sw9AVszv1SzTqcc0eccwnOuTLOuTLAYi5R2EVEskrj8gl88XQj3r2/JmfSztFldAp3D1vE0m2HvI6WrQLVrENEJGhERBi3VUtfI//PDlXZcegEdw9bxGMjl7J+b85YI6+9ZUQk7J08k8aIhVsZOnsLx06n0qF6CXq1rkDJQnm8jnbZtHGYiEgGv5w4w9A5Wxi5YBvnXPoa+e4ty5GQN3TWyKu4i4hcwJ4jJxn07SbGpewiNiqCLk3K0rVpWfLmivI62iWpuIuIXMLmfcfo//UGpq7eS+G4GLq3LMf99UqRKyp495FXcRcR8dOKnb/wxrT1LNxykKSCuendugLtq5cIyn3ktZ+7iIifkkvG8+8u9RjzWF3i80TTe9wKbhs0j5nrfw7ZfeRV3EVESN9iuEn5RCY/3ZjB99Xg1Nk0Hh2ZQqf3FrFse+itkVdxFxE5T0SEcXtycWb0bsbf76jCtoMnuGvoIrqMSmHD3l+9juc3zbmLiFzEiTOpjFiwjWFz0tfI31kjiV6ty5NU0Js18gF7Q9XMYoG5QC4gChjvnHslw5jeQBcgFdgPPOqc236x86q4i0goOXz8DMPmbGHEwm3goHP99DXyheJisjVHIIv7JZt1mFkLYIlz7oSZPQk0d87dc7HzqriLSCjac+QkA2dNdKDoAAAGVUlEQVRs4rNlO8kTE0XXJmXp0uRa4rJpjXy2Nutwzs1yzv3W72ox6VsDi4iEnWIFcvNGx2p83aspjcslMOCbjTR7cxajFm7jTOo5r+P9V0CadWTwGDA1EOFERIJVuSL5GPZALSY+1ZByRfLyyuQ1tHprNpOW7+ZcEOwjf1lvqPq2/p0I9HDOrc7keGegO9DMOXc6k+PdgG4ApUqVqrV9+0Wn5UVEQoJzjrmbDvDG1PWs3XOUSkXz8WKbSjSvmBjwfeSz7BOqGZt1nPfzG4HBpBf2fZc6j+bcRSTcnDvnmLJqD/2/3sD2gyeoe20hXmxTiVqlCwbsPrKtWYfv5zWA90hv0nHJwi4iEo4iIox2ycWZ0asZr7WvzI/7j3PX0IV0HZ3Cpp+zd428P6tlqgGjgEjSnwzGOedeNbNXgRTn3GQz+waoCuzx/doO59xFG3nolbuIhLsTZ1L5cP5W3pvzI8fPpHJnzSR6ta5AifjcV3xObRwmIhIkDh8/w5DZmxm1KP19xhdurkiXJmWv6FzaOExEJEgUjIvhz7fdwKznmnNH9eLZ0gEq+HemFxEJEyXic9O3Y3K23JdeuYuIhCEVdxGRMKTiLiIShlTcRUTCkIq7iEgYUnEXEQlDKu4iImFIxV1EJAx5tv2Ame0HrnTP3wTgQADjBEqw5oLgzaZcl0e5Lk845irtnEu81CDPivvVMLMUf/ZWyG7BmguCN5tyXR7lujw5OZemZUREwpCKu4hIGArV4j7c6wAXEKy5IHizKdflUa7Lk2NzheScu4iIXFyovnIXEZGLCOribmZtzGyDmW02sz9mcjyXmX3qO77EzMoESa6HzWy/mf3g++qSTbk+NLN9Zrb6AsfNzAb5cq80s5pBkqu5mR0573r9JRsylTSzWWa2zszWmNkzmYzJ9uvlZ65sv16++401s+/MbIUv298yGZPtj0k/c3n1mIw0s+VmNiWTY1l7rZxzQflFes/WLUBZIAZYAdyQYcxTwDDf7XuBT4Mk18PAOx5cs6ZATWD1BY7fCkwFDKgPLAmSXM2BKdl8rYoBNX238wEbM/nvmO3Xy89c2X69fPdrQF7f7WhgCVA/wxgvHpP+5PLqMdkbGJvZf6+svlbB/Mq9LrDZOfejc+4M8AnQPsOY9qQ37wYYD7QyMwuCXJ5wzs0FDl1kSHtgtEu3GIg3s2JBkCvbOef2OOe+993+FVgHlMgwLNuvl5+5POG7Dsd830b7vjK+aZftj0k/c2U7M0sCbgPev8CQLL1WwVzcSwA7z/t+F7//n/y/Y5xzqcARoHAQ5AK4y/en/HgzK5nFmfzlb3YvNPD9WT3VzCpn5x37/hyuQforvvN5er0ukgs8ul6+aYYfgH3ADOfcBa9ZNj4m/ckF2f+YHAi8AJy7wPEsvVbBXNwzewbL+Gzsz5hA8+c+vwTKOOeqAd/wf8/OXvPievnje9I/Up0MDAYmZdcdm1leYALwrHPuaMbDmfxKtlyvS+Ty7Ho559Kcc9WBJKCumVXJMMSTa+ZHrmx9TJpZW2Cfc27ZxYZl8rOAXatgLu67gPOfXZOAny40xsyigAJk/Z//l8zlnDvonDvt+/ZfQK0szuQvf65ptnPOHf3tz2rn3FdAtJklZPX9mlk06QX03865zzMZ4sn1ulQur65Xhgy/ALOBNhkOefGYvGQuDx6TjYB2ZraN9Knblmb2UYYxWXqtgrm4LwXKm9m1ZhZD+hsOkzOMmQw85LvdEZjpfO9OeJkrw7xsO9LnTYPBZOBB3yqQ+sAR59wer0OZWdHf5hrNrC7p/18ezOL7NOADYJ1z7q0LDMv26+VPLi+ul+++Es0s3nc7N3AjsD7DsGx/TPqTK7sfk865l5xzSc65MqTXiJnOuc4ZhmXptYoK1IkCzTmXambdgemkr1D50Dm3xsxeBVKcc5NJfxCMMbPNpD/j3RskuXqaWTsg1Zfr4azOBWBmH5O+kiLBzHYBr5D+5hLOuWHAV6SvANkMnAAeCZJcHYEnzSwVOAncmw1P0o2AB4BVvrlagD8Bpc7L5cX18ieXF9cL0lfyjDKzSNKfUMY556Z4/Zj0M5cnj8mMsvNa6ROqIiJhKJinZURE5AqpuIuIhCEVdxGRMKTiLiIShlTcRUTCkIq7iEgYUnEXEQlDKu4iImHo/wFnH2gW4aQiigAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_size = 16\n",
    "batch_size = 10\n",
    "encoder1 = EncoderRNN(output_lang.n_words, hidden_size, 1, batch_size=batch_size).to(device)\n",
    "attn_decoder1 = LuongAttnDecoderRNN('general', hidden_size, output_lang.n_words, 1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 500, print_every=50, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 小公主兩眼緊緊地盯著金球，可是金球忽地一下子在水潭裡就沒影兒了。\n",
      "= 因為水潭裡的水很深，看不見底，小公主就哭了起來，她的哭聲越來越大，哭得傷心極了。\n",
      "< 小 公主 就 她 的 呀 。 」 的 呀 。 」 的 呀 了 。 」 裡 ， 她 的 哭聲 越來 她 的 呀 。 」 隻 越來 她 的 越來 她 的 呀 了 。 」 隻 越來 她 的 越來 她 的 越來 她 的 越來 她 的 呀 了 。 」 隻 越來 她 的 呀 。 」 裡 ， 她 的 哭聲 越來 她 的 越來 她 的 哭聲 越來 她 的 哭聲 越來 她 的 哭聲 越來 她 的 越來 她 的 越來 。 」 的 呀 ， 小 公主 小 公主 就 哭 了 。 」 隻 越來 她 的 越來 她 的 哭聲 傷心極 了 。 」 隻 越來 她 的 哭聲 越來 她 的 哭聲 越來 她 的 越來 她 的 越來 她 的 呀 ， 小 公主 就 很 的 呀 。 」 的 呀 。 」 的 呀 。 」 裡 ， 她 的 哭聲 越來 她 的 越來 哭 。 」 隻 越來 」 隻 越來 她 的 越來 她 的 越來 她 的 哭聲 越來 她 的 哭聲 越來 她 的 越來 她 的 越來 她 的 哭聲 越來 她 的 哭聲 越來 她 的 哭聲\n",
      "\n",
      "> 馬車來接年輕的王子回他的王國去。\n",
      "= 忠心耿耿的亨利扶著他的主人和王妃上了車廂，然後自己又站到了車後邊去。\n",
      "< 對 對 對 對 突然 的 亨利 的 時候 王妃 。 。 。 。 。 。 了 。 。 。 。 。 。 後 ， 聽見 了 車後邊 是 ， 聽見 了 站 。 。 。 。 後 ， 突然 他 的 時候 王妃 了 了 國王 。 。 。 。 」 裡 ， 聽見 啦 。 我 王妃 。 。 。 。 。 。 。 。 」 裡 ， 聽見 了 車後邊 是 。 。 。 。 。 。 。 。 。 。 。 了 。 。 。 。 」 隻 亨利 的 時候 王妃 。 。 。 。 。 。 。 。 。 。 後 ， 「 的 王妃 。 。 。 。 。 。 。 」 的 時候 箍 公主 王妃 。 是 ， 聽見 了 了 國王 。 。 。 。 。 。 」 的 時候 王妃 。 。 。 。 。 。 。 。 」 。 」 隻 美麗 的 亨利 出來 。 」 隻 亨利 的 王國 。 」 的 時候 箍 王子 著 亨利 。 。 。 。 什麼 了 。 。 。 。 。 。 」 的 時候 箍 公主 著 金球 吐 裡 ， 聽見 了 車後邊 是 了 。 。\n",
      "\n",
      "> 小公主對青蛙的喊叫根本不予理睬，而是逕直跑回了家，並且很快就把可憐的青蛙忘記得一乾二淨。\n",
      "= 青蛙只好蹦蹦跳跳地又回到水潭裡去。\n",
      "< 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小 小\n",
      "\n",
      "> 小公主回答說，「不是什麼巨人，而是一隻討厭的青蛙。」\n",
      "= 「青蛙想找你做什麼呢？」\n",
      "< 她 的 做 。 」 裡去 」 裡去 了 。 」 裡去 了 ， 就 掉 到 了 她 的 做 。 」 裡去 了 。 」 隻 做 就 撈 出來 ， 就 ， 就 撈 出來 ， 就 哭 了 。 」 裡去 了 ， 就 哭 了 。 」 裡去 了 。 」 我 撈 他 。 」 隻 做 ， 就 ， 就 掉 撈 金球 撈 出來 ， 就 哭 了 。 」 裡去 ， 就 ， 就 撈 出來 ， 就 就 ， 就 掉 掉 到 青蛙 我 ， 就 撈 他 的 朋友 」 ， 就 撈 出來 ， 就 ， 就 掉 掉 到 青蛙 我 ， 青蛙 就 撈 出來 ， 就 ， 就 撈 出來 ， 就 哭 了 。 」 裡去 」 的 做 ， 就 掉 哭 就 掉 掉 她 的 做 。 」 裡去 」 裡去 了 。 」 裡去 ， 就 哭 了 。 」 裡去 」 裡去 了 。 」 裡去 」 隻 做 ， 就 掉 到 就 撈 出來 ， 就 ， 就 撈 出來 ， 就 ， 就 ， 就 撈 出來 ， 就 ， 就 掉 就 撈 出來\n",
      "\n",
      "> 儘管青蛙扯著嗓子拚命叫喊，可是沒有一點兒用。\n",
      "= 小公主對青蛙的喊叫根本不予理睬，而是逕直跑回了家，並且很快就把可憐的青蛙忘記得一乾二淨。\n",
      "< 「 您 的 金球 ， 青蛙 金球 ， 金球 吧 。 」 的 ， 金球 ， 金球 吧 。 」 在 東西 的 ， 金球 吧 。 」 的 忘記 。 」 什麼 」 的 忘記 。 」 ， 金球 吧 。 」 在 東西 的 時候 ， 青蛙 金球 撈 他 。 ， 他 的 朋友 ， 王子 。 」 在 東西 的 王國 。 」 ， 金球 ， 金球 吧 。 。 」 ， 去 金球 吧 。 。 」 在 東西 的 ， 和 推過 ， 青蛙 青蛙 王國 。 」 裡去 。 」 的 忘記 了 。 。 」 ， 金球 吧 。 」 ， 去 是 」 ， 的 朋友 ， 王子 。 了 。 。 」 的 忘記 。 」 的 忘記 了 。 。 。 」 在 東西 的 王國 。 」 在 東西 的 金球 ， 金球 掉 到 了 公主 的 金球 ， 金球 ， 金球 吧 。 ， 他 的 王國 。 」 在 東西 的 王國 。 」 ， 金球 吧 。 」 在 東西 的 王國 。 」 在 東西 的 王國 。 」 的 忘記 。 」 的 忘記 。 」 ， 去 金球 掉 。 」\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"「好的，太好了，」\")\n",
    "plt.matshow(attentions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"je sais\")\n",
    "\n",
    "# evaluateAndShowAttention(\"sans façons !\")\n",
    "\n",
    "# evaluateAndShowAttention(\"Il n'en est pas question !\")\n",
    "\n",
    "# evaluateAndShowAttention(\"En aucune manière !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
